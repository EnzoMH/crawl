from fastapi import FastAPI, WebSocket, APIRouter, WebSocketDisconnect
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from starlette.responses import FileResponse
from pydantic import BaseModel
from typing import List, Dict
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.common.action_chains import ActionChains
from slack_sdk import WebClient
from slack_sdk.errors import SlackApiError
from threading import Timer
from datetime import datetime, timedelta
from ipchal_gonggo import router as ipchal_router # ë¼ìš°í„° ì„í¬íŠ¸ (ë‹¤ë¥¸ íŒŒì¼ì—ì„œ ë¼ìš°í„°ë¥¼ ê°€ì ¸ì˜´)

import asyncio, json, gc, uvicorn, webbrowser, os

app = FastAPI(title="ì‚¬ì „ê·œê²©ê³µê°œ + ì…ì°°ê³µê³  í¬ë¡¤ë§ API")
router = APIRouter()
# CORS ë¯¸ë“¤ì›¨ì–´ ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=['*'],
    allow_headers=["*"],
)

# ipchal_gonggo.py ë¼ìš°í„° ë“±ë¡ (ì£¼ì–´ì§„ prefix ì‚¬ìš©)
app.include_router(ipchal_router, prefix="/api")

# ì •ì  íŒŒì¼ ì„œë¹™ (assets ë””ë ‰í† ë¦¬ë¥¼ '/assets' ê²½ë¡œì— ë§¤í•‘)
app.mount("/assets", StaticFiles(directory="static/assets"), name="assets")

SLACK_TOKEN = "xoxb-7772790073142-7779533740258-faMs1MjHOqHhCQM92NIci5d3"
slack_client = WebClient(token=SLACK_TOKEN)

# í¬ë¡¤ë§í•  ê²€ìƒ‰ì–´ ë¦¬ìŠ¤íŠ¸
SEARCH_TERMS = [
    "LEDê²½ê´€ì¡°ëª…ê¸°êµ¬",
    "ê²½ê´€ì¡°ëª…ê¸°êµ¬",
    "êµìœ¡ìš©ë¡œë´‡",
    "ë™ì˜ìƒì œì‘ì„œë¹„ìŠ¤",
    "ë””ìì¸ì„œë¹„ìŠ¤",
    "ì•ˆë‚´ì „ê´‘íŒ",
    "ì˜ìƒì •ë³´ë””ìŠ¤í”Œë ˆì´ì¥ì¹˜",
    "ì¡°ëª…ìš©ì œì–´ì¥ì¹˜",
    "ì¡°í˜•ë¬¼",
    "ì‹¤ë¬¼ëª¨í˜•ë°ì „ì‹œë¬¼",
]

class CrawlingState:
    def __init__(self):
        self.is_running = False                     # í¬ë¡¤ë§ ì‹¤í–‰ ì—¬ë¶€
        self.current_term = ""                      # í˜„ì¬ ì§„í–‰ ì¤‘ì¸ ê²€ìƒ‰ì–´
        self.active_connections: List[WebSocket] = []  # í™œì„± WebSocket ëª©ë¡
        self.crawler_task = None
        self.last_crawl_time = None
        self.next_crawl_time = None

        self.collected_data: List[Dict] = []        # í¬ë¡¤ë§ìœ¼ë¡œ ëª¨ì€ ë°ì´í„°
        self.scheduler_task = None                  # í¬ë¡¤ë§ì„ ì£¼ê¸°ì ìœ¼ë¡œ ì‹¤í–‰í•˜ëŠ” íƒœìŠ¤í¬
        self.completed_terms: List[str] = []        # ì´ë¯¸ ì™„ë£Œëœ ê²€ìƒ‰ì–´ ëª©ë¡
        self.current_term_index = 0                 # í˜„ì¬ ì§„í–‰ ì¤‘ì¸ ê²€ìƒ‰ì–´ ì¸ë±ìŠ¤
        
    def save_progress(self):
        if self.current_term in SEARCH_TERMS:
            current_index = SEARCH_TERMS.index(self.current_term)
            self.completed_terms = SEARCH_TERMS[:current_index]
            self.current_term_index = current_index
        
    def restore_progress(self):
        if self.completed_terms:
            self.current_term_index = len(self.completed_terms)
            return True
        return False

crawling_state = CrawlingState() # ì „ì—­ ìƒíƒœ ì¸ìŠ¤í„´ìŠ¤

def cleanup_resources(driver=None):
    if driver:
        driver.quit()
    gc.collect()


@app.websocket("/ws")
async def websocket_endpoint(websocket: WebSocket):
    await websocket.accept()
    crawling_state.active_connections.append(websocket)
    try:
        while True:
            await websocket.receive_text()
            if not crawling_state.is_running:
                break
            # í˜„ì¬ê¹Œì§€ ìˆ˜ì§‘ëœ ë°ì´í„° ì „ì†¡
            await websocket.send_json({
                "type": "data",
                "data": crawling_state.collected_data
            })
    except Exception as e:
        print(f"WebSocket error: {e}")
    finally:
        crawling_state.active_connections.remove(websocket)


async def broadcast_message(message: dict):
    for connection in crawling_state.active_connections:
        try:
            await connection.send_json(message)
        except:
            crawling_state.active_connections.remove(connection)

@app.websocket("/api/ws")
async def websocket_endpoint(websocket: WebSocket):
    await websocket.accept()
    crawling_state.active_connections.append(websocket)

    try:
        await websocket.send_json({
            "type": "status",
            "data": {
                "is_running": crawling_state.is_running,
                "remaining_time": crawling_state.get_remaining_time(),
                "forms_data": crawling_state.forms_data
            }
        })
        while True:
            await websocket.receive_text()
    except WebSocketDisconnect:
        crawling_state.active_connections.remove(websocket)
    except Exception as e:
        print(f"WebSocket error: {e}")
        if websocket in crawling_state.active_connections:
            crawling_state.active_connections.remove(websocket)

class WebDriver:
    def __init__(self):
        self.driver = None
        self.wait = None
    
    def setup(self):
        chrome_options = Options()
        chrome_options.add_argument('--headless=new')
        chrome_options.add_argument('--disable-gpu')
        chrome_options.add_argument('--disable-extensions')
        chrome_options.add_argument('--no-sandbox')
        chrome_options.add_argument('--disable-dev-shm-usage')
        chrome_options.add_argument('--memory-pressure-off')
        chrome_options.add_argument('--window-size=1920,1080')
        chrome_options.add_argument('--start-maximized')
        chrome_options.add_argument('--disable-blink-features=AutomationControlled')
        chrome_options.add_experimental_option('excludeSwitches', ['enable-automation'])
        chrome_options.add_experimental_option('useAutomationExtension', False)
        
        service = Service('C:\\Users\\admin\\Downloads\\chromedriver-win64\\chromedriver.exe')
        self.driver = webdriver.Chrome(service=service, options=chrome_options)
        self.wait = WebDriverWait(self.driver, 10)

        self.driver.set_window_size(1920, 1080)
        return self.driver, self.wait

class PopupHandler:
    @staticmethod
    async def handle_popups(driver, wait):
        if not crawling_state.is_running:
            return
        first_close_css = "div.w2window_close[title='ì°½ë‹«ê¸°']"
        first_close_button = wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, first_close_css)))
        first_close_button.click()
        print("ì²« ë²ˆì§¸ íŒì—… ë‹«ê¸° ì„±ê³µ")
            
        if not crawling_state.is_running:
            return
        await asyncio.sleep(2)

class NavigationHandler:
    def __init__(self, driver, wait):
        self.driver = driver
        self.wait = wait
        self.main_window = None
    
    async def navigate_to_pre_spec(self):
        if not crawling_state.is_running:
            return
            
        try:
            balju = self.wait.until(EC.element_to_be_clickable(
                (By.ID, "mf_wfm_gnb_wfm_gnbMenu_genDepth1_0_btn_menuLvl1_span"))
            )
            balju.click()
            
            if not crawling_state.is_running:
                return
            await asyncio.sleep(2)
            
            balju_list = self.wait.until(EC.element_to_be_clickable(
                (By.ID, "mf_wfm_gnb_wfm_gnbMenu_genDepth1_0_genDepth2_0_btn_menuLvl2_span"))
            )
            balju_list.click()
            
            if not crawling_state.is_running:
                return
            await asyncio.sleep(2)
            
            pre_spec = self.wait.until(EC.element_to_be_clickable(
                (By.ID, "mf_wfm_container_radSrchTy_input_1"))
            )
            self.driver.execute_script("arguments[0].click();", pre_spec)
            
            if not crawling_state.is_running:
                return
            await asyncio.sleep(2)
            
        except Exception as e:
            print(f"ë„¤ë¹„ê²Œì´ì…˜ ì¤‘ ì˜¤ë¥˜: {e}")
            raise e

class SearchHandler:
    def __init__(self, driver, wait):
        self.driver = driver
        self.wait = wait
        self.main_window = None
    
    async def search_product(self, search_term):
        if not crawling_state.is_running:
            return
            
        try:
            # ìƒì„¸ì¡°ê±´ í´ë¦­
            detail_condition = self.wait.until(EC.element_to_be_clickable(
                (By.CSS_SELECTOR, "input.w2trigger.btn_cm.srch_toggle[value='ìƒì„¸ì¡°ê±´']"))
            )
            self.driver.execute_script("arguments[0].click();", detail_condition)
            
            if not crawling_state.is_running:
                return
            await asyncio.sleep(2)
            
            # 'ë¬¼í’ˆ' ì²´í¬ë°•ìŠ¤
            products = self.wait.until(EC.element_to_be_clickable(
                (By.ID, 'mf_wfm_container_chkRqdcBsneSeCd_input_1'))
            )
            self.driver.execute_script("arguments[0].click();", products)
            
            if not crawling_state.is_running:
                return
            await asyncio.sleep(2)
            
            self.main_window = self.driver.current_window_handle
            
            # 'ì„¸ë¶€í’ˆëª…' ê²€ìƒ‰(ë‹ë³´ê¸°)
            magnifier = self.wait.until(EC.element_to_be_clickable(
                (By.ID, 'mf_wfm_container_btnPrnmNoPoup2'))
            )
            self.driver.execute_script("arguments[0].click();", magnifier)
            
            if not crawling_state.is_running:
                return
            await asyncio.sleep(2)
            
            # ê²€ìƒ‰ì–´ ì…ë ¥
            spec_products = self.wait.until(EC.element_to_be_clickable(
                (By.ID, 'mf_wfm_container_DtlsPrnmSrchPL_wframe_popupCnts_ibxDtlsPrnm'))
            )
            spec_products.click()
            spec_products.send_keys(search_term)
            
            if not crawling_state.is_running:
                return
            await asyncio.sleep(2)
            
            # ê²€ìƒ‰ ë²„íŠ¼ í´ë¦­
            spec_magnifier = self.wait.until(EC.element_to_be_clickable(
                (By.ID, 'mf_wfm_container_DtlsPrnmSrchPL_wframe_popupCnts_btnS0001'))
            )
            spec_magnifier.click()
            
            if not crawling_state.is_running:
                return
            await asyncio.sleep(2)
            
            await self.handle_search_results(search_term)
            
        except Exception as e:
            print(f"ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}")
            raise e

    async def handle_search_results(self, search_term):
        if not crawling_state.is_running:
            return
            
        try:
            cells_base_xpath = (
                '//*[contains(@id, "mf_wfm_container_DtlsPrnmSrchPL_wframe_popupCnts_grdDtlsPrnm_cell_") '
                'and contains(@id, "_1")]'
            )
            cells = self.driver.find_elements(By.XPATH, cells_base_xpath)
            
            matching_cell = None
            for cell in cells:
                if cell.text == search_term:
                    matching_cell = cell
                    print(f"ì •í™•íˆ ì¼ì¹˜í•˜ëŠ” í•­ëª© '{search_term}' ì°¾ìŒ")
                    break
            
            if matching_cell:
                actions = ActionChains(self.driver)
                actions.double_click(matching_cell).perform()
                
                if not crawling_state.is_running:
                    return
                await asyncio.sleep(2)
            else:
                raise Exception(f"ê²€ìƒ‰ì–´ '{search_term}'ì™€ ì¼ì¹˜í•˜ëŠ” ì„¸ë¶€í’ˆëª…ì´ ì—†ìŠµë‹ˆë‹¤.")
            
            await self.handle_windows()
            await self.perform_final_search()
            
        except Exception as e:
            print(f"ê²€ìƒ‰ ê²°ê³¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            raise e

    async def handle_windows(self):
        if not crawling_state.is_running:
            return
            
        try:
            all_windows = self.driver.window_handles
            for window in all_windows:
                if window != self.main_window:
                    self.driver.switch_to.window(window)
                    
                    if not crawling_state.is_running:
                        return
                    await asyncio.sleep(2)
                    
                    self.driver.close()
            
            self.driver.switch_to.window(self.main_window)
            
        except Exception as e:
            print(f"ìœˆë„ìš° ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            raise e

    async def perform_final_search(self):
        if not crawling_state.is_running:
            return
            
        try:
            final_magnifier = self.wait.until(EC.element_to_be_clickable(
                (By.ID, 'mf_wfm_container_btnS0001'))
            )
            final_magnifier.click()
            
            if not crawling_state.is_running:
                return
            await asyncio.sleep(2)
            
        except Exception as e:
            print(f"ìµœì¢… ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}")
            raise e

class DataExtractor:
    def __init__(self, driver, wait, search_term):
        self.driver = driver
        self.wait = wait
        self.search_term = search_term
        self.slack_client = slack_client

    async def extract_detail_info(self):
        if not crawling_state.is_running:
            return None
        try:
            fields = {
                "ì‚¬ì „ê·œê²©ë“±ë¡ë²ˆí˜¸": "//input[@class='w2input df_input w2input_readonly' and @title='ì‚¬ì „ê·œê²©ë“±ë¡ë²ˆí˜¸']",
                "ì‚¬ì „ê·œê²©ëª…": "//label[@class='w2textbox ' and @style='width:calc(100%)']",
                "ìˆ˜ìš”ê¸°ê´€": "//input[@class='w2input df_input w2input_readonly' and @title='ìˆ˜ìš”ê¸°ê´€']",
                "ë°°ì •ì˜ˆì‚°ì•¡": "//input[@class='w2input df_input tar w2input_readonly' and @title='ë°°ì •ì˜ˆì‚°ì•¡']",
                "ì˜ê²¬ë“±ë¡ë§ˆê°ì¼ì‹œ": "//input[@class='w2input df_input w2input_readonly' and @style='width:200px;']",
                "ì‚¬ì „ê·œê²©ê³µê°œì¼ì‹œ": "//input[@id='mf_wfm_container_alotBgtPrspAmt']"
            }
            detail_info = {"search_term": self.search_term}
            
            for field_name, xpath in fields.items(): # ê° í•„ë“œ í•˜ë‚˜ì”© ì°¾ì•„ì„œ ê°’ ì¶”ì¶œ
                if not crawling_state.is_running:
                    return None
                try:
                    element = self.wait.until(EC.presence_of_element_located((By.XPATH, xpath)))
                    value = element.get_attribute("value") if field_name != "ì‚¬ì „ê·œê²©ëª…" else element.text # ì‚¬ì „ê·œê²©ëª…ì€ .text, ë‚˜ë¨¸ì§€ëŠ” value
                    print(f"í•„ë“œ {field_name}: {value}")
                    detail_info[field_name] = value
                except Exception as e:
                    print(f"{field_name} í•„ë“œ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
                    detail_info[field_name] = None
            
            # ì‚¬ì „ê·œê²©ê³µê°œì¼ì‹œê°€ ì˜¤ëŠ˜ì´ë©´ Slack ì•Œë¦¼
            if detail_info.get('ì‚¬ì „ê·œê²©ê³µê°œì¼ì‹œ'):
                today = datetime.now().strftime('%Y%m%d')
                notice_date = detail_info['ì‚¬ì „ê·œê²©ê³µê°œì¼ì‹œ'].split()[0]
                
                if notice_date == today:
                    await self.send_slack_notification(detail_info)
            
            return detail_info
                
        except Exception as e:
            print(f"ìƒì„¸ ì •ë³´ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
            return None

    async def send_slack_notification(self, detail_info):
        try:
            slack_message = (
                f"ğŸ”” ìƒˆë¡œìš´ ì‚¬ì „ê·œê²© ê³µê³  ğŸ””\n\n"
                f"ì‚¬ì „ê·œê²©ëª…: {detail_info['ì‚¬ì „ê·œê²©ëª…']}\n"
                f"ì‚¬ì „ê·œê²©ë“±ë¡ë²ˆí˜¸: {detail_info['ì‚¬ì „ê·œê²©ë“±ë¡ë²ˆí˜¸']}\n"
                f"ìˆ˜ìš”ê¸°ê´€: {detail_info['ìˆ˜ìš”ê¸°ê´€']}\n"
                f"ë°°ì •ì˜ˆì‚°ì•¡: {detail_info['ë°°ì •ì˜ˆì‚°ì•¡']}ì›\n"
                f"ì˜ê²¬ë“±ë¡ë§ˆê°ì¼ì‹œ: {detail_info['ì˜ê²¬ë“±ë¡ë§ˆê°ì¼ì‹œ']}\n"
            )
            
            response = self.slack_client.chat_postMessage(
                channel="C07NR1N7NNA",
                text=slack_message
            )
            
            if response['ok']:
                print(f"Slack ì•Œë¦¼ ì „ì†¡ ì™„ë£Œ: {detail_info['ì‚¬ì „ê·œê²©ë“±ë¡ë²ˆí˜¸']}")
            else:
                print(f"Slack ì•Œë¦¼ ì „ì†¡ ì‹¤íŒ¨: {response['error']}")
        except Exception as e:
            print(f"Slack ì•Œë¦¼ ì „ì†¡ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

    async def extract_all_items(self):
        if not crawling_state.is_running:
            return []
            
        collected_results = []
        try:
            # ëª©ë¡ ì¤‘ ì‚¬ì „ê·œê²©ëª…ì„ í´ë¦­í•˜ëŠ” ë¶€ë¶„(_11)
            cells_xpath = "//td[contains(@id, 'mf_wfm_container_gridView1_cell_') and contains(@id, '_11')]/nobr/a"
            cells = self.driver.find_elements(By.XPATH, cells_xpath)
            
            # í‘œì‹œë˜ê³ , í…ìŠ¤íŠ¸ê°€ ìˆëŠ” ì…€ë§Œ ì¹´ìš´íŠ¸
            total_items = len([cell for cell in cells if cell.is_displayed() and cell.text.strip()])
            
            if total_items == 0:
                return collected_results
            
            # ê° ì•„ì´í…œì„ í•˜ë‚˜ì”© ì—´ì–´ ìƒì„¸ ì •ë³´ ì¶”ì¶œ
            for i in range(total_items):
                if not crawling_state.is_running:
                    return collected_results
                    
                try:
                    current_xpath = f'//*[@id="mf_wfm_container_gridView1_cell_{i}_11"]/nobr/a'
                    if self.driver.find_element(By.XPATH, current_xpath).is_displayed():
                        link_elem = self.wait.until(EC.element_to_be_clickable((By.XPATH, current_xpath)))
                        self.driver.execute_script("arguments[0].click();", link_elem)
                        
                        if not crawling_state.is_running:
                            return collected_results
                        await asyncio.sleep(2)
                        
                        # ìƒì„¸ ì¶”ì¶œ
                        result = await self.extract_detail_info()
                        if result:
                            collected_results.append(result)

                        # ë‹¤ì‹œ ëª©ë¡ìœ¼ë¡œ
                        self.driver.back()
                        
                        if not crawling_state.is_running:
                            return collected_results
                        await asyncio.sleep(2)
                        
                except Exception as e:
                    print(f"{i+1}ë²ˆì§¸ í•­ëª© ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                    continue
                    
        except Exception as e:
            print(f"ë°ì´í„° ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        
        return collected_results

async def perform_crawling():
    try:
        if not crawling_state.is_running:
            return

        # ë§ˆì§€ë§‰/ë‹¤ìŒ í¬ë¡¤ë§ ì‹œê°„ ê¸°ë¡
        crawling_state.last_crawl_time = datetime.now()
        crawling_state.next_crawl_time = crawling_state.last_crawl_time + timedelta(hours=1)
        
        # ì´ì „ ì§„í–‰ ìƒíƒœ ë³µì› í›„, ë‚¨ì€ ê²€ìƒ‰ì–´ë§Œ ìˆœíšŒ
        start_index = crawling_state.current_term_index
        search_terms = SEARCH_TERMS[start_index:]
        
        for search_term in search_terms:
            if not crawling_state.is_running:
                crawling_state.save_progress()
                break
                
            # ìƒˆë¡œìš´ ë“œë¼ì´ë²„ ì„¸íŒ…
            web_driver = WebDriver()
            driver, wait = web_driver.setup()
            
            try:
                crawling_state.current_term = search_term
                # ì§„í–‰ë¥  ê³„ì‚°
                current_progress = ((start_index + search_terms.index(search_term) + 1) / len(SEARCH_TERMS)) * 100
                
                # í˜„ì¬ ê²€ìƒ‰ì–´, ì§„í–‰ë¥  ë“± WebSocketìœ¼ë¡œ ë¸Œë¡œë“œìºìŠ¤íŠ¸
                await broadcast_message({
                    "type": "status",
                    "data": f"{search_term} í¬ë¡¤ë§ ì¤‘... (ì§„í–‰ë¥ : {current_progress:.1f}%)"
                })
                
                # ì‚¬ì´íŠ¸ ì ‘ì†
                driver.get('https://www.g2b.go.kr')
                
                if not crawling_state.is_running:
                    crawling_state.save_progress()
                    break
                await asyncio.sleep(2)
                
                # íŒì—… ì²˜ë¦¬
                popup_handler = PopupHandler()
                await popup_handler.handle_popups(driver, wait)
                
                if not crawling_state.is_running:
                    crawling_state.save_progress()
                    break
                
                # í˜ì´ì§€ ì´ë™
                navigation = NavigationHandler(driver, wait)
                await navigation.navigate_to_pre_spec()
                
                if not crawling_state.is_running:
                    crawling_state.save_progress()
                    break
                
                # ìƒì„¸ ê²€ìƒ‰
                search_handler = SearchHandler(driver, wait)
                await search_handler.search_product(search_term)
                
                if not crawling_state.is_running:
                    crawling_state.save_progress()
                    break
                
                # ë°ì´í„° ì¶”ì¶œ
                data_extractor = DataExtractor(driver, wait, search_term)
                results = await data_extractor.extract_all_items()
                
                if results:
                    crawling_state.collected_data.extend(results)
                    await broadcast_message({
                        "type": "data",
                        "data": results
                    })
                
                # ê²€ìƒ‰ì–´ ì™„ë£Œ ì²˜ë¦¬
                if search_term not in crawling_state.completed_terms:
                    crawling_state.completed_terms.append(search_term)
                crawling_state.current_term_index = SEARCH_TERMS.index(search_term) + 1
                
                if not crawling_state.is_running:
                    crawling_state.save_progress()
                    break
                await asyncio.sleep(2)
                
            except Exception as e:
                print(f"{search_term} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                crawling_state.save_progress()
                await broadcast_message({
                    "type": "error",
                    "data": f"{search_term} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}"
                })
                
            finally:
                cleanup_resources(driver)
        
        # ëª¨ë“  ê²€ìƒ‰ì–´ë¥¼ ì²˜ë¦¬í•œ ê²½ìš° ìƒíƒœ ì´ˆê¸°í™”
        if crawling_state.current_term_index >= len(SEARCH_TERMS):
            crawling_state.current_term_index = 0
            crawling_state.completed_terms = []
                    
        await broadcast_message({
            "type": "complete",
            "data": {
                "message": "í¬ë¡¤ë§ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.",
                "lastCrawlTime": crawling_state.last_crawl_time.strftime("%Y-%m-%d %H:%M:%S"),
                "nextCrawlTime": crawling_state.next_crawl_time.strftime("%Y-%m-%d %H:%M:%S"),
                "completedTerms": crawling_state.completed_terms,
                "progress": f"{(len(crawling_state.completed_terms) / len(SEARCH_TERMS)) * 100:.1f}%"
            }
        })
            
    except Exception as e:
        print(f"Perform crawling error: {e}")
        crawling_state.save_progress()
        await broadcast_message({
            "type": "error",
            "data": str(e)
        })

async def scheduled_crawling():
    while True:
        try:
            if not crawling_state.is_running:
                break
            await perform_crawling()
            for _ in range(360): # ì •í™•íˆ 1ì‹œê°„ ëŒ€ê¸° (10ì´ˆì”© 360ë²ˆ)
                if not crawling_state.is_running:
                    break
                await asyncio.sleep(10)
                
        except asyncio.CancelledError: # íƒœìŠ¤í¬ê°€ ì·¨ì†Œë˜ë©´ ë£¨í”„ ì¢…ë£Œ
            break
        except Exception as e:
            print(f"Scheduled crawling error: {e}")
            if crawling_state.is_running:
                await asyncio.sleep(60)  # ì—ëŸ¬ ë°œìƒ ì‹œ 1ë¶„ í›„ ì¬ì‹œë„

@app.get("/{full_path:path}")
async def serve_app(full_path: str):
    return FileResponse("static/index.html")

@app.post("/api/start")
async def start_crawling():
    if not crawling_state.is_running:
        crawling_state.is_running = True
        if crawling_state.scheduler_task is None:
            crawling_state.scheduler_task = asyncio.create_task(scheduled_crawling())
        return {"message": "í¬ë¡¤ë§ì´ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤."}
    return {"message": "í¬ë¡¤ë§ì´ ì´ë¯¸ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤."}


@app.post("/api/stop")
async def stop_crawling():
    if crawling_state.is_running:
        crawling_state.is_running = False
        crawling_state.save_progress()
        if crawling_state.scheduler_task:
            crawling_state.scheduler_task.cancel()
            crawling_state.scheduler_task = None
        return {
            "message": "í¬ë¡¤ë§ì´ ì¤‘ì§€ë˜ì—ˆìŠµë‹ˆë‹¤.",
            "progress": {
                "completed_terms": crawling_state.completed_terms,
                "current_term": crawling_state.current_term
            }
        }
    return {"message": "í¬ë¡¤ë§ì´ ì‹¤í–‰ ì¤‘ì´ ì•„ë‹™ë‹ˆë‹¤."}


@app.get("/api/status")
async def get_status():
    return {
        "is_running": crawling_state.is_running,
        "current_term": crawling_state.current_term,
        "collected_data": crawling_state.collected_data,
        "last_crawl_time": (crawling_state.last_crawl_time.strftime("%Y-%m-%d %H:%M:%S")
                            if crawling_state.last_crawl_time else None),
        "next_crawl_time": (crawling_state.next_crawl_time.strftime("%Y-%m-%d %H:%M:%S")
                            if crawling_state.next_crawl_time else None),
        "completed_terms": crawling_state.completed_terms,
        "progress_percentage": (
            (len(crawling_state.completed_terms) / len(SEARCH_TERMS)) * 100
            if crawling_state.completed_terms else 0
        )
    }

@app.get("/api/results")
async def get_results():
    return {"results": crawling_state.collected_data}

def open_browser():
    webbrowser.open("http://192.168.132.216:8000")

if __name__ == "__main__":
    Timer(1.5, open_browser).start() # 1.5ì´ˆ í›„ ë¸Œë¼ìš°ì € ìë™ ì˜¤í”ˆ
    uvicorn.run("pre_spec:app", host="0.0.0.0", port=8000)